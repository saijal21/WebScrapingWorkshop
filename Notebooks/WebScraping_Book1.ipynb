{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a8d6a7-9825-4bc0-9612-0ab05e34942f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4650d4-99d4-4ebb-989a-63d39b1124bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the HTML file\n",
    "with open('C:\\\\Users\\\\shahania\\\\Documents\\\\Phd\\\\HERSS Summer School\\\\Tutorial\\\\Test_HTML_1.html', 'r', encoding='utf-8') as file:\n",
    "    html_content = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b8ea55-3006-4a54-a866-f3612c4a8b92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Print the parsed HTML with proper indentation\n",
    "print(soup)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7dd6acd8-2a73-4c8b-94f3-19905dc472fb",
   "metadata": {},
   "source": [
    "**Handing Malformed HTML**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10530885-2d95-4271-8078-768ec5f835a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "malformed_html = '''\n",
    "<section>\n",
    "    <h2>Content Section</h2>\n",
    "    <p class=\"content\">Here is some <b>bold</b> text and <i>italic</i> text.</p>\n",
    "    <p class=\"content\">Another paragraph with a <a href=\"http://example.com\">link</a>.</p>\n",
    "    <p class=\"content\">This paragraph is missing a closing tag for <i>italic text.\n",
    "</section>\n",
    "'''\n",
    "\n",
    "soup = BeautifulSoup(malformed_html, 'html.parser')\n",
    "print(soup.prettify())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec861722-e734-41e6-b4ec-e68162d3be88",
   "metadata": {},
   "source": [
    "**Search and Extract Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c316c8b0-fd69-497e-b3cd-32225446d7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Extract the title\n",
    "title = soup.title.string\n",
    "print(f\"Title: {title}\")\n",
    "\n",
    "# Find all paragraphs\n",
    "paragraphs = soup.find_all('p')\n",
    "for i, p in enumerate(paragraphs, start=1):\n",
    "    print(f\"Paragraph {i}: {p.get_text()}\")\n",
    "\n",
    "# Find and print the link\n",
    "link = soup.find('a')\n",
    "print(f\"Link Text: {link.get_text()}, URL: {link['href']}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0399b3-ed9a-482e-8e04-b81a8fdcec8c",
   "metadata": {},
   "source": [
    "**HTML/XML parsing + Integration with Parsers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fe3a39-e68a-4829-aad9-9d79f17121f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup_lxml = BeautifulSoup(html_content, 'lxml')\n",
    "print(f\"Title using lxml: {soup_lxml.title.string}\")\n",
    "\n",
    "# Alternatively, parse the HTML content with the default html.parser\n",
    "soup_html_parser = BeautifulSoup(html_content, 'html.parser')\n",
    "print(f\"Title using html.parser: {soup_html_parser.title.string}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd01ff16-5115-409c-9a37-27f8cde4a566",
   "metadata": {},
   "source": [
    "**Text Manipulation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a489a995-a6d5-42e3-8ff5-7eefe73ffe72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the title\n",
    "soup.title.string = \"Updated Sample Web Page Title\"\n",
    "\n",
    "# Add a new paragraph\n",
    "new_paragraph = soup.new_tag('p')\n",
    "new_paragraph.string = \"This is a new paragraph added to the content section.\"\n",
    "soup.body.append(new_paragraph)\n",
    "\n",
    "# Print the modified HTML\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa143af-651d-4d11-bc9f-d129702289f8",
   "metadata": {},
   "source": [
    "**Let's extract differnt HTML tags**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2282c53a-5171-4a77-8b2e-5a971d05f6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the HTML content\n",
    "with open('C:\\\\Users\\\\shahania\\\\Documents\\\\Phd\\\\HERSS Summer School\\\\Tutorial\\\\HTML_TAGS.html', 'r') as file:\n",
    "    soup = BeautifulSoup(file, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0107775d-b84c-486a-af30-461e2f10d327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Scraping Images\n",
    "def scrape_images(soup):\n",
    "    print(\"=== Scraping Images ===\")\n",
    "    images = soup.find_all('img')\n",
    "    for img in images:\n",
    "        img_url = img['src']\n",
    "        img_name = img['alt'].replace(' ', '_') + \".jpg\"\n",
    "        img_data = requests.get(img_url).content\n",
    "        with open(img_name, 'wb') as handler:\n",
    "            handler.write(img_data)\n",
    "        print(f\"Downloaded {img_name}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077579cf-2518-42ad-88ae-e80d1fdbbcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_images(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe996418-a83c-40bb-ad48-9e8607804fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Scraping PDFs\n",
    "def scrape_pdfs(soup):\n",
    "    print(\"=== Scraping PDFs ===\")\n",
    "    pdf_links = soup.find_all('a', href=True)\n",
    "    for link in pdf_links:\n",
    "        if link['href'].endswith('.pdf'):\n",
    "            pdf_url = link['href']\n",
    "            pdf_name = link.get_text().replace(' ', '_') + \".pdf\"\n",
    "            pdf_data = requests.get(pdf_url).content\n",
    "            with open(pdf_name, 'wb') as handler:\n",
    "                handler.write(pdf_data)\n",
    "            print(f\"Downloaded {pdf_name}\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0480d354-71b5-4670-a7a4-6d74598201cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_pdfs(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1819deb-dd38-456d-b930-a52348e176ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Scraping Lists (Unordered and Ordered)\n",
    "def scrape_lists(soup):\n",
    "    print(\"=== Scraping Unordered Lists (ul) ===\")\n",
    "    ul_items = soup.find_all('ul')\n",
    "    for ul in ul_items:\n",
    "        list_items = ul.find_all('li')\n",
    "        for item in list_items:\n",
    "            print(f\"Unordered list item: {item.get_text()}\")\n",
    "\n",
    "    print(\"=== Scraping Ordered Lists (ol) ===\")\n",
    "    ol_items = soup.find_all('ol')\n",
    "    for ol in ol_items:\n",
    "        ordered_items = ol.find_all('li')\n",
    "        for step in ordered_items:\n",
    "            print(f\"Ordered list step: {step.get_text()}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cbbfcf-fd27-4200-b4bc-82de5eb502df",
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_lists(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20467793-4a0d-468e-a5ce-9be87689305e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Scraping Table Data (only cells)\n",
    "def scrape_tables(soup):\n",
    "    print(\"=== Scraping Table Data ===\")\n",
    "    table = soup.find('table')\n",
    "    rows = table.find_all('tr')\n",
    "\n",
    "    for row in rows:\n",
    "        cells = row.find_all('td')\n",
    "        for cell in cells:\n",
    "            print(f\"Table Cell: {cell.get_text()}\")\n",
    "    print(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6b315d-684e-4ff5-88db-ad58ed04f68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete\n",
    "def scrape_tables(soup):\n",
    "    print(\"=== Scraping Table Data ===\")\n",
    "    \n",
    "    # Find the table in the HTML\n",
    "    table = soup.find('table')\n",
    "    \n",
    "    # Extract the header (if present)\n",
    "    headers = table.find_all('th')\n",
    "    header_row = [header.get_text().strip() for header in headers]\n",
    "    print(f\"Table Headers: {header_row}\")\n",
    "    \n",
    "    # Extract all the rows\n",
    "    rows = table.find_all('tr')\n",
    "    \n",
    "    # Loop through each row\n",
    "    for row in rows:\n",
    "        # Extract individual cells\n",
    "        cells = row.find_all('td')\n",
    "        cell_data = [cell.get_text().strip() for cell in cells]\n",
    "        \n",
    "        # If the row has data, print it\n",
    "        if cell_data:\n",
    "            print(f\"Row Data: {cell_data}\")\n",
    "            \n",
    "        # Print the entire row (optional to use later for row-level processing)\n",
    "        entire_row = ' | '.join(cell_data)\n",
    "        print(f\"Entire Row: {entire_row}\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81618b6a-540f-4eb2-8acd-12c38bcfe81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_tables(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b1a819-2cc9-49dc-8b6f-ce7f42e5d7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Scraping Navigation Links\n",
    "def scrape_links(soup):\n",
    "    print(\"=== Scraping Navigation Links ===\")\n",
    "    links = soup.find_all('a')\n",
    "    for link in links:\n",
    "        print(f\"Link Text: {link.get_text()}, URL: {link['href']}\")\n",
    "    print(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47449f6d-4f42-47ef-b444-c7afbd73e3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_links(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6821d1-06f6-4142-bf3f-c035049b8566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Scraping Headings (h1, h2, etc.)\n",
    "def scrape_headings(soup):\n",
    "    print(\"=== Scraping Headings ===\")\n",
    "    headings = soup.find_all(['h1', 'h2'])\n",
    "    for heading in headings:\n",
    "        print(f\"{heading.name}: {heading.get_text()}\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec9b424-89b7-421a-b1b5-0b0e11ff178d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_headings(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e6fe7f-a197-405d-9eae-b49a7634c4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 7. Handling Attributes (for instance, class or id)\n",
    "def scrape_attributes(soup):\n",
    "    print(\"=== Scraping Elements with Class or ID ===\")\n",
    "    elements_with_class = soup.find_all(class_=\"item\")\n",
    "    for elem in elements_with_class:\n",
    "        print(f\"Element with class 'item': {elem.get_text()}\")\n",
    "\n",
    "    elements_with_id = soup.find_all(id=True)\n",
    "    for elem in elements_with_id:\n",
    "        print(f\"Element with ID: {elem.get_text()}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02d0263-e1ed-409a-8dee-970ba2b4a9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_attributes(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2448f589-7de0-4d8f-aa9b-f519acfd0225",
   "metadata": {},
   "source": [
    "**Let's try with the website**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139b2a44-791c-4faf-885f-09e2ca8a071e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/128.0.0.0 Safari/537.36'\n",
    "} # \n",
    "url = 'https://www.goodreads.com/'\n",
    "response = requests.get(url,headers=headers)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a5585c-2636-49d9-9b92-8857fe15b5c0",
   "metadata": {},
   "source": [
    "Beautiful Soup does not handle HTTP requests or handle JavaScript. It focuses entirely on parsing HTML/XML documents. You typically use it in combination with libraries like requests to fetch web pages and then use Beautiful Soup to extract the data you need from those pages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9faeca86-c589-4f71-8431-d29226415477",
   "metadata": {},
   "source": [
    "**Headers, specifically the User-Agent header, play a crucial role in making your HTTP requests appear more like those from a regular browser.** \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac0a2f8-241c-4c3b-8ac0-8997edecb05a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "soup.prettify()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c69c4d6-3ffc-457f-ab56-68b2eb68a401",
   "metadata": {},
   "source": [
    "If the content is missing, inspect the source of the Goodreads page in your browser (right-click and select \"View Page Source\") and compare it with what you get from requests. If they differ, it means the content is being rendered by JavaScript after the initial page load. That's where Selenium comes in!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c3a16f-d96e-46eb-8e27-ab37aaf739bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Using find() to find the first occurrence of a specific tag or class\n",
    "# Let's find the title of the page\n",
    "page_title = soup.find('title').get_text()\n",
    "print(f\"Page Title: {page_title}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5e9ce4-d0ad-49c0-9daa-803bf3fe63c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Using find_all() to find all occurrences of a tag\n",
    "# Find all the links in the footer\n",
    "footer_links = soup.find_all('a', class_='responsiveSiteFooter__link')\n",
    "print(\"Footer Links:\")\n",
    "for link in footer_links:\n",
    "    print(link.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b09276f-b4f8-455b-b65d-8e453507f35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Using select() to find elements using CSS selectors\n",
    "# Find the main promo headline using a CSS selector\n",
    "promo_headline = soup.select('div.promoHeader__promoMastheadContent h2')[0].get_text()\n",
    "print(f\"Promo Headline: {promo_headline}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c581c9e9-47cd-48c7-8e8f-d755252ab6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Using parent to navigate to the parent element\n",
    "# Find the parent of the first footer link\n",
    "first_footer_link = soup.find('a', class_='responsiveSiteFooter__link')\n",
    "footer_parent = first_footer_link.parent\n",
    "print(f\"Parent of the first footer link: {footer_parent.name}\")  # This should print the name of the parent tag, e.g., 'li'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4b18b6-0434-4692-8e15-c8b026bc9f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Using next_sibling to navigate to the next sibling\n",
    "# Find the next sibling after the first footer link\n",
    "next_sibling = first_footer_link.find_next_sibling()\n",
    "print(f\"Next sibling after the first footer link: {next_sibling.get_text()}\")  # Should be the text of the next link"
   ]
  },
  {
   "cell_type": "raw",
   "id": "21f53787-77af-4f60-aa7a-a42d29a46ca7",
   "metadata": {},
   "source": [
    "Because we have no sibling here. \n",
    "\n",
    "*What is a sibling?*\n",
    "\n",
    "<div class=\"family\">\r\n",
    "    <div class=\"parent\">\r\n",
    "        <p>Child 1</p>\r\n",
    "        <p>Child 2</p>\r\n",
    "        <p>Chil/p>d\n",
    "   </div>\n",
    "</div>>\r\n",
    "<\n",
    "\n",
    "Parent:\n",
    "\n",
    "The element directly above another element in the hierarchy.\n",
    "In the example, the <div class=\"parent\"> is the parent of <p> elements (Child 1, Ch ild 2 Chil\n",
    "\n",
    "Children:\n",
    "\n",
    "Elements directly below another element in the hierarchy.\n",
    "The <p> elements are children of the <div class=\"parent\">.\n",
    "\n",
    "\n",
    "Siblings:\n",
    "\n",
    "Elements that share the same parent.\n",
    "In the example, \"Child 1\", \"Child 2\", and \"Child 3\" are siblings because they all share the same parent <div =\"parent\"ssnpare\n",
    "/div>\r\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e54f889-deac-4deb-aae7-1f21238d3c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the first genre link\n",
    "first_genre = soup.find('a', href=\"/genres/art\")\n",
    "print(f\"First genre: {first_genre.get_text()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4eba728-964d-4b02-8b89-9f66f35b11c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the next sibling genre link\n",
    "next_genre = first_genre.find_next_sibling('a')\n",
    "print(f\"Next genre: {next_genre.get_text()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab04564-2274-43b6-8749-9466e246aeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the next sibling again\n",
    "third_genre = next_genre.find_next_sibling('a')\n",
    "print(f\"Third genre: {third_genre.get_text()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cea563-2947-4f40-9611-3387cc450d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Using previous_sibling to navigate to the previous sibling\n",
    "# Find the previous sibling of the next sibling (should bring us back to the first link)\n",
    "previous_sibling = third_genre.find_previous_sibling('a')\n",
    "print(f\"Previous sibling of the next link: {previous_sibling.get_text()}\")  # Should be the text of the first link again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2407629-ef32-4f7b-9146-b0949cd9df1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Using previous_sibling to navigate to the previous sibling\n",
    "# Find the previous sibling of the next sibling (should bring us back to the first link)\n",
    "first_genre_again  = previous_sibling.find_previous_sibling('a')\n",
    "print(f\"Previous sibling of the next link: {first_genre_again.get_text()}\")  # Should be the text of the first link again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ac801f-492e-4238-ac21-a1d41b85c107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Using find_parent to find a specific parent element\n",
    "# Find the parent of a specific link using find_parent\n",
    "specific_link = soup.find('a', href='/about/us')\n",
    "specific_parent = specific_link.find_parent('ul')\n",
    "print(f\"Specific parent of 'About Us' link: {specific_parent.name}\")  # Should be 'ul'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bbdcbb-47da-4116-ba1a-d6c0488f8f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Using find_all to find all images with a specific class\n",
    "# Find all book images in the feature teaser boxes\n",
    "book_images = soup.find_all('img', class_='bookImgSimilar')\n",
    "print(\"Book Images:\")\n",
    "for img in book_images:\n",
    "    print(img['src'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadbf8ac-8b72-43aa-a1f2-1d71fbdbfa79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Using select to find elements by attribute\n",
    "# Use a CSS selector to find an element by its attribute (e.g., data-react-class)\n",
    "react_components = soup.select('[data-react-class]')\n",
    "print(\"React Components with data-react-class:\")\n",
    "for component in react_components:\n",
    "    print(component['data-react-class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5afefc-aa10-457d-95fd-62db64855388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Using find to find an element with a specific text\n",
    "# Find a link containing specific text\n",
    "best_quotes_link = soup.find('a', string='Best quotes')\n",
    "print(f\"Best quotes link: {best_quotes_link['href']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a817a30e-3617-4f24-ac4a-c85ad810cd88",
   "metadata": {},
   "source": [
    "**When to use what**?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc68ad10-6486-45cc-b213-4494ee68a271",
   "metadata": {},
   "source": [
    "1. find()\n",
    "When to Use: Use find() when you want to locate the first occurrence of a specific tag or element that matches your criteria.\n",
    "2. find_all()\n",
    "When to Use: Use find_all() when you need to retrieve all elements that match a specific tag or criteria.\n",
    "3. select()\n",
    "When to Use: Use select() when you need to find elements using complex CSS selectors. It’s very powerful for matching based on classes, IDs, or nested elements.\n",
    "\n",
    "4. find_parent() and parent\n",
    "When to Use: Use these methods to navigate upwards in the DOM tree to find the parent of a specific element.\n",
    "5. find_next_sibling() and find_previous_sibling()\n",
    "When to Use: Use these methods to navigate sideways in the DOM tree to find the next or previous sibling of an element. It’s useful when dealing with elements at the same level. (Example: Finding the next and previous paragraphs around a specific paragraph)\n",
    "\n",
    "6. children\n",
    "When to Use: Use children when you want to iterate over all direct children of an element. This is useful for navigating elements that contain nested elements.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
